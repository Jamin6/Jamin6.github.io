<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/jamin6/2023/11/02/%E6%9D%A1%E4%BB%B6%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(CGAN)/"/>
      <url>/jamin6/2023/11/02/%E6%9D%A1%E4%BB%B6%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(CGAN)/</url>
      
        <content type="html"><![CDATA[<h1 id="条件生成对抗网络-CGAN"><a href="#条件生成对抗网络-CGAN" class="headerlink" title="条件生成对抗网络(CGAN)"></a>条件生成对抗网络(CGAN)</h1><h1 id="一、CGAN"><a href="#一、CGAN" class="headerlink" title="一、CGAN"></a>一、CGAN</h1><h2 id="1、什么是CGAN"><a href="#1、什么是CGAN" class="headerlink" title="1、什么是CGAN"></a>1、什么是CGAN</h2><p>条件生成对抗网络（Conditional Generative Adversarial Nets），为GANdd的改进版本。通过为数据增加label (y)进行构造，在G和D的输入上都增加了label。然后做了两个有条件的GAN的实验。也就是给定条件y,结合随机分布，生成符合条件y的样本。</p><h2 id="2、原始GAN存在的问题"><a href="#2、原始GAN存在的问题" class="headerlink" title="2、原始GAN存在的问题"></a>2、原始GAN存在的问题</h2><p>①、之前介绍的无条件的GAN中，生成的数据是不可控的，代码运行结果也可以看出，只能生成0-9的数字，但无法保证是哪一个。但是本文给定标签的CGAN网络，可以基于class label生成特定数字的图像。</p><p>②、对于one-to-many mapping模型，比如image tag问题，一张图像可能不止一个tag,传统模型无法解决，因此可以使用条件生成概率，将输入图像视为conditional variable(条件变量)，使用条件预测分布去获取一对多的映射关系。</p><h2 id="3、CGAN的应用"><a href="#3、CGAN的应用" class="headerlink" title="3、CGAN的应用"></a>3、CGAN的应用</h2><p>CGAN可以应用于图像修补，多模态深度学习等任务当中。目前在这些领域也已经有很深入的应用。</p><h2 id="4、CGAN流程图"><a href="#4、CGAN流程图" class="headerlink" title="4、CGAN流程图"></a>4、CGAN流程图</h2><p><img src="../../../../../Tools/Typora/images/image-20230714154956447.png" alt="image-20230714154956447"></p><h2 id="5、目标函数"><a href="#5、目标函数" class="headerlink" title="5、目标函数"></a>5、目标函数</h2><p><img src="../../../../../Tools/Typora/images/image-20230714155155176.png" alt="image-20230714155155176"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习中常用的方法(函数)</title>
      <link href="/jamin6/2023/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95(%E5%87%BD%E6%95%B0)/"/>
      <url>/jamin6/2023/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95(%E5%87%BD%E6%95%B0)/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习中常用的方法-函数"><a href="#机器学习中常用的方法-函数" class="headerlink" title="机器学习中常用的方法(函数)"></a>机器学习中常用的方法(函数)</h1><p>本篇文章主要编写一些在机器学习中经常使用的方法，以便索引。</p><h2 id="view"><a href="#view" class="headerlink" title="view()"></a>view()</h2><p><code>view()</code> 是 PyTorch 张量（Tensor）的一个方法，用于改变张量的形状而不改变其数据。它类似于 Numpy 的 <code>reshape()</code> 方法。通过调用 <code>view()</code> 方法，可以将一个张量重新变换为另一种形状，但需要满足两个条件：</p><ol><li>变换后的形状与原始形状包含相同数量的元素。</li><li>变换后的形状在各维度上保持相同大小的连续性，即不能跨越内存块。</li></ol><p><code>view()</code> 方法的参数是一个表示目标形状的元组或列表。常用的参数如下：</p><ol><li><strong>-1</strong>：自动推断维度大小。可以在形状中的某个位置使用 -1，PyTorch 会根据其他维度的大小自动计算该位置的大小。</li><li><strong>整数</strong>：指定该维度的大小为给定的整数值。</li><li><strong>0</strong>：保持当前维度不变。可以在形状中的某个位置使用 0，以保持对应维度的大小不变。</li></ol><p>使用方法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tensor = tensor.view(shape)</span><br></pre></td></tr></table></figure><p>其中 <code>tensor</code> 是要进行形状变换的张量，<code>shape</code> 是一个元组或列表，表示变换后的形状。新的张量 <code>new_tensor</code> 将具有指定的形状，并且与原始张量共享相同的数据存储（只是形状视图的不同）。</p><p>以下是一些示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># 创建一个形状为 (4, 3, 2) 的张量</span></span><br><span class="line"><span class="built_in">print</span>(x.size())  <span class="comment"># 输出: torch.Size([4, 3, 2])</span></span><br><span class="line"></span><br><span class="line">y = x.view(<span class="number">12</span>, <span class="number">2</span>)  <span class="comment"># 改变形状为 (12, 2)，相当于展平了前两维</span></span><br><span class="line"><span class="built_in">print</span>(y.size())  <span class="comment"># 输出: torch.Size([12, 2])</span></span><br><span class="line"></span><br><span class="line">z = x.view(-<span class="number">1</span>)  <span class="comment"># 改变形状为一维，自动推断维度</span></span><br><span class="line"><span class="built_in">print</span>(z.size())  <span class="comment"># 输出: torch.Size([24])</span></span><br><span class="line"></span><br><span class="line">w = x.view(<span class="number">6</span>, -<span class="number">1</span>)  <span class="comment"># 改变形状为 (6, -1)，其中 -1 的大小将自动计算为 4</span></span><br><span class="line"><span class="built_in">print</span>(w.size())  <span class="comment"># 输出: torch.Size([6, 4])</span></span><br></pre></td></tr></table></figure><p>在使用 <code>view()</code> 进行形状变换时，需要确保变换后的形状是合法的，否则会引发错误。另外，由于 <code>view()</code> 方法返回的是视图（view）而不是新的张量对象，因此对返回的张量进行修改会同时影响原始张量。如果需要得到一个新的张量对象，可以使用 <code>clone()</code> 方法创建一个副本。</p><p>注意：<code>view()</code> 方法并不会改变张量的存储方式，即如果对返回的张量进行修改，可能会影响原始张量。如果需要在内存中重新排列张量的数据，可以使用 <code>reshape()</code> 方法。</p><h2 id="eye"><a href="#eye" class="headerlink" title="eye()"></a>eye()</h2><p><code>该方法在创建独热编码时经常使用。</code></p><p><code>eye()</code> 是 PyTorch 的一个方法，用于创建一个单位矩阵（identity matrix）。单位矩阵是一个方阵，其对角线上的元素全为 1，其他元素全为 0。<code>eye()</code> 方法的作用是创建一个指定大小的单位矩阵。</p><p><code>eye()</code> 方法的参数如下：</p><ul><li><code>n</code>：单位矩阵的行数和列数，即矩阵的大小。</li></ul><p>使用方法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (3, 3) 的单位矩阵</span></span><br><span class="line">x = torch.eye(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>输出结果为:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">复制代码tensor([[1., 0., 0.],</span><br><span class="line">        [0., 1., 0.],</span><br><span class="line">        [0., 0., 1.]])</span><br></pre></td></tr></table></figure><p>上述代码中，<code>torch.eye(3)</code> 创建了一个形状为 (3, 3) 的单位矩阵。该张量的对角线上的元素全为 1，其他元素全为 0。</p><p><code>eye()</code> 方法还可以通过设置额外参数来控制单位矩阵的性质，例如：</p><ul><li><code>dtype</code>：指定创建的单位矩阵的数据类型。</li><li><code>device</code>：指定创建的单位矩阵的存储设备。</li></ul><p>以下是一些示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (4, 4) 的单位矩阵，数据类型为 float32</span></span><br><span class="line">x = torch.eye(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为 (2, 2) 的单位矩阵，在 CUDA 设备上存储</span></span><br><span class="line">y = torch.eye(<span class="number">2</span>, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure><p>通过指定不同的参数，可以根据需要创建具有不同性质的单位矩阵。</p><h2 id="cat"><a href="#cat" class="headerlink" title="cat()"></a>cat()</h2><p><code>torch.cat()</code> 方法用于按指定维度拼接（连接）多个张量。它将多个输入张量在给定的维度上进行连接，返回一个新的张量。</p><p><code>torch.cat()</code> 方法的参数包括：</p><ul><li><code>tensors</code>：要拼接的张量序列，可以是一个元组、列表或张量的迭代器。</li><li><code>dim</code>：指定在哪个维度上进行拼接操作。</li></ul><p>使用方法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个形状为 (2, 3) 的张量</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按行拼接（在维度 0 上进行拼接）</span></span><br><span class="line">result = torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按列拼接（在维度 1 上进行拼接）</span></span><br><span class="line">result = torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br></pre></td></tr></table></figure><p>在上述示例中，我们创建了两个形状为 (2, 3) 的张量 x 和 y。然后，我们使用 <code>torch.cat()</code> 方法按不同的维度进行拼接操作。在第一个例子中，我们在维度 0 上进行拼接，即按行拼接，得到了形状为 (4, 3) 的新张量。在第二个例子中，我们在维度 1 上进行拼接，即按列拼接，得到了形状为 (2, 6) 的新张量。</p><p>需要注意的是，<code>torch.cat()</code> 方法要求除了拼接维度之外，其他维度上的大小必须完全一致。否则，将引发错误。另外，<code>dim</code> 参数必须是有效的维度索引，它应该是 0 到 <code>tensors[0].dim()-1</code> 的值之一。</p><p>此外，<code>torch.cat()</code> 方法也支持拼接非张量变量，如 Python 列表或元组。只需将它们作为 <code>tensors</code> 参数传递给 <code>torch.cat()</code> 即可。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个形状为 (2, 2) 的列表</span></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">y = [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接列表（在维度 0 上进行拼接）</span></span><br><span class="line">result = torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure><p>通过 <code>torch.cat()</code> 方法，我们可以方便地在指定维度上拼接多个张量或变量，以满足不同的数据操作需求。</p><h3 id="dim参数"><a href="#dim参数" class="headerlink" title="dim参数"></a>dim参数</h3><p><code>dim</code> 是 <code>torch.cat()</code> 等方法中的一个参数，用于指定在哪个维度上进行相关操作，这里以cat方法作为参考。</p><p>在张量中，维度表示张量的阶数或轴的数量。例如，对于形状为 (3, 4, 5) 的三维张量，它有 3 个维度，分别是维度 0、维度 1 和维度 2。</p><p>当我们使用 <code>torch.cat()</code> 方法拼接多个张量时，需要指定拼接的维度。这个维度决定了拼接后张量的形状。</p><p>假设我们有两个形状相同的张量 <code>x</code> 和 <code>y</code>，它们的形状为 (m, n)，那么在 <code>torch.cat((x, y), dim=0)</code> 中：</p><ul><li>如果 <code>dim=0</code>，则在维度 0 上进行拼接，沿着行的方向将 <code>x</code> 和 <code>y</code> 连接起来。结果的形状将是 (2m, n)，即行数变为原来的两倍。</li><li>如果 <code>dim=1</code>，则在维度 1 上进行拼接，沿着列的方向将 <code>x</code> 和 <code>y</code> 连接起来。结果的形状将是 (m, 2n)，即列数变为原来的两倍。</li></ul><p>需要注意的是，具体的维度索引值取决于张量的维度数量。对于一个形状为 (m, n, p) 的三维张量：</p><ul><li>如果 <code>dim=0</code>，则在维度 0 上进行拼接，结果的形状将是 (2m, n, p)。</li><li>如果 <code>dim=1</code>，则在维度 1 上进行拼接，结果的形状将是 (m, 2n, p)。</li><li>如果 <code>dim=2</code>，则在维度 2 上进行拼接，结果的形状将是 (m, n, 2p)。</li></ul><p>通过指定不同的 <code>dim</code> 值，我们可以在不同的维度上对张量进行拼接，从而实现灵活的数据操作。</p><h2 id="transforms-Resize"><a href="#transforms-Resize" class="headerlink" title="transforms.Resize()"></a>transforms.Resize()</h2><p><code>transforms.Resize()</code>是PyTorch中的一个图像预处理操作，用于调整图像的大小。它的作用是将输入的图像调整为指定的大小。</p><p>在使用<code>transforms.Resize()</code>时，你可以提供以下参数：</p><ul><li><code>size</code>：它可以是一个整数、一个元组或列表、或者一个数字序列。如果<code>size</code>是一个整数n，图像的最短边将被调整为n像素，并且长宽比将被保持。如果<code>size</code>是一个长度为2的元组<code>(h, w)</code>，图像将被调整为高度h和宽度w。如果<code>size</code>是一个整数列表或数字序列，其中每个整数表示一个要调整的边长，图像将按顺序调整为这些边长。</li></ul><p>下面是<code>transforms.Resize()</code>的使用方法示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像预处理</span></span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),  <span class="comment"># 将图像调整为256x256的大小</span></span><br><span class="line">    transforms.ToTensor()   <span class="comment"># 转换为张量</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并预处理图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)  <span class="comment"># 请将&#x27;image.jpg&#x27;替换成实际的图像文件路径</span></span><br><span class="line">image = preprocess(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印调整后的图像大小</span></span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br></pre></td></tr></table></figure><p>在上述示例中，<code>transforms.Resize((256, 256))</code>将图像调整为256x256的大小。然后，通过<code>transforms.ToTensor()</code>将图像转换为张量。最后，打印调整后的图像大小。</p><p>另外，你还可以使用其他参数形式来调整图像的大小，例如<code>transforms.Resize(256)</code>将图像的最短边调整为256像素，并保持长宽比；或者使用<code>transforms.Resize([256, 512])</code>按顺序调整图像的宽度为256像素，高度为512像素。</p><h2 id="transforms-CenterCrop"><a href="#transforms-CenterCrop" class="headerlink" title="transforms.CenterCrop()"></a>transforms.CenterCrop()</h2><p><code>transforms.CenterCrop()</code>是PyTorch中的一个图像预处理操作，用于将图像居中裁剪到指定的大小。它的作用是保留图像的中心部分，并将其调整为指定的大小。</p><p>在使用<code>transforms.CenterCrop()</code>时，你可以提供以下参数：</p><ul><li><code>size</code>：它可以是一个整数或一个元组。如果<code>size</code>是一个整数n，图像将被裁剪为高度和宽度都是n的正方形。如果<code>size</code>是一个长度为2的元组<code>(h, w)</code>，图像将被裁剪为高度h和宽度w的矩形。</li></ul><p>下面是<code>transforms.CenterCrop()</code>的使用方法示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像预处理</span></span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),  <span class="comment"># 将图像居中裁剪为224x224的大小</span></span><br><span class="line">    transforms.ToTensor()   <span class="comment"># 转换为张量</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并预处理图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)  <span class="comment"># 请将&#x27;image.jpg&#x27;替换成实际的图像文件路径</span></span><br><span class="line">image = preprocess(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印裁剪后的图像大小</span></span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br></pre></td></tr></table></figure><p>在上述示例中，<code>transforms.CenterCrop(224)</code>将图像居中裁剪为224x224的大小。然后，通过<code>transforms.ToTensor()</code>将图像转换为张量。最后，打印裁剪后的图像大小。</p><h2 id="transforms-Normalize"><a href="#transforms-Normalize" class="headerlink" title="transforms.Normalize()"></a>transforms.Normalize()</h2><p><code>transforms.Normalize()</code>是PyTorch中的一个图像预处理操作，用于对图像进行标准化处理。它的作用是将图像的每个通道的像素值按照给定的均值和标准差进行标准化，以便在模型训练过程中更好地收敛。</p><p>在使用<code>transforms.Normalize()</code>时，你可以提供以下参数：</p><ul><li><code>mean</code>：一个表示均值的元组或列表。长度必须等于图像通道数。该参数用于对图像的每个通道进行均值减法。</li><li><code>std</code>：一个表示标准差的元组或列表。长度必须等于图像通道数。该参数用于对图像的每个通道进行标准差除法。</li></ul><p>下面是<code>transforms.Normalize()</code>的使用方法示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均值和标准差</span></span><br><span class="line">mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像预处理</span></span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),   <span class="comment"># 转换为张量</span></span><br><span class="line">    transforms.Normalize(mean, std)  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并预处理图像</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)  <span class="comment"># 请将&#x27;image.jpg&#x27;替换成实际的图像文件路径</span></span><br><span class="line">image = preprocess(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印标准化后的图像通道均值和标准差</span></span><br><span class="line"><span class="built_in">print</span>(image.mean(dim=(<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="built_in">print</span>(image.std(dim=(<span class="number">1</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>在上述示例中，<code>transforms.ToTensor()</code>将图像转换为张量。然后，通过<code>transforms.Normalize(mean, std)</code>对图像的每个通道进行标准化，其中<code>mean</code>和<code>std</code>分别表示图像通道的均值和标准差。最后，打印标准化后的图像通道均值和标准差。</p><p>需要注意的是，在使用<code>transforms.Normalize()</code>之前，图像通常应该通过<code>transforms.ToTensor()</code>将其转换为张量。</p><h2 id="torch-unsqueeze"><a href="#torch-unsqueeze" class="headerlink" title="torch.unsqueeze()"></a>torch.unsqueeze()</h2><p><code>torch.unsqueeze()</code>是PyTorch中的一个函数，用于在张量的指定位置增加一个维度。它的作用是将原始张量扩展为更高维度的张量。</p><p>在使用<code>torch.unsqueeze()</code>时，你可以提供以下参数：</p><ul><li><code>input</code>：要进行维度扩展的输入张量。</li><li><code>dim</code>：一个整数，表示在哪个位置插入新的维度。该参数的取值范围是从0到输入张量的维度数。</li></ul><p>下面是<code>torch.unsqueeze()</code>的使用方法示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个二维张量</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用torch.unsqueeze()在第二个维度处增加一个维度</span></span><br><span class="line">x_new0 = torch.unsqueeze(x, dim=<span class="number">0</span>)</span><br><span class="line">x_new1 = torch.unsqueeze(x, dim=<span class="number">1</span>)</span><br><span class="line">x_new2 = torch.unsqueeze(x, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印原始张量和扩展后的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;扩展后的张量：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_new0)</span><br><span class="line"><span class="built_in">print</span>(x_new1)</span><br><span class="line"><span class="built_in">print</span>(x_new2)</span><br></pre></td></tr></table></figure><p>在上述示例中，我们首先创建了一个二维张量<code>x</code>。然后，使用<code>torch.unsqueeze(x, dim=1)</code>在第二个维度处插入一个新的维度。最后，打印原始张量和扩展后的张量。</p><p>输出结果如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原始张量：</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]])</span><br><span class="line">扩展后的张量：</span><br><span class="line">tensor([[[1, 2, 3],</span><br><span class="line">         [4, 5, 6]]])</span><br><span class="line">tensor([[[1, 2, 3]],</span><br><span class="line"></span><br><span class="line">        [[4, 5, 6]]])</span><br><span class="line">tensor([[[1],</span><br><span class="line">         [2],</span><br><span class="line">         [3]],</span><br><span class="line"></span><br><span class="line">        [[4],</span><br><span class="line">         [5],</span><br><span class="line">         [6]]])</span><br></pre></td></tr></table></figure><p>在扩展后的张量中，原来的维度1被扩展为了维度2。</p><h2 id="torch-squeeze"><a href="#torch-squeeze" class="headerlink" title="torch.squeeze()"></a>torch.squeeze()</h2><p><code>torch.squeeze()</code>是PyTorch中的一个函数，用于去除张量中维度为1的维度。它的作用是将原始张量中维度为1的维度进行压缩，从而减少张量的维度数。</p><p>在使用<code>torch.squeeze()</code>时，你可以提供以下参数：</p><ul><li><code>input</code>：要进行维度压缩的输入张量。</li><li><code>dim</code>：一个整数或元组，表示要压缩的维度。该参数是可选的。如果指定了<code>dim</code>，则只会在指定的维度处进行压缩，而不是在所有维度为1的维度处进行压缩。</li></ul><p>下面是<code>torch.squeeze()</code>的使用方法示例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个具有维度为1的额外维度的张量</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用torch.squeeze()压缩张量的维度为1的维度</span></span><br><span class="line">x_new = torch.squeeze(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印原始张量和压缩后的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始张量：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;压缩后的张量：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_new)</span><br></pre></td></tr></table></figure><p>在上述示例中，我们首先创建了一个张量<code>x</code>，它具有维度为1的额外维度。然后，使用<code>torch.squeeze(x)</code>压缩张量的维度为1的维度。最后，打印原始张量和压缩后的张量。</p><p>输出结果如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原始张量：</span><br><span class="line">tensor([[[[ 1.3378, -0.6038, -0.5162, -1.0522, -0.9087]],</span><br><span class="line"></span><br><span class="line">         [[-0.5057, -1.8196, -0.1545, -1.2484,  0.7989]],</span><br><span class="line"></span><br><span class="line">         [[ 1.2472, -0.8905,  1.4315, -1.1237, -0.8593]]]])</span><br><span class="line">压缩后的张量：</span><br><span class="line">tensor([[ 1.3378, -0.6038, -0.5162, -1.0522, -0.9087],</span><br><span class="line">        [-0.5057, -1.8196, -0.1545, -1.2484,  0.7989],</span><br><span class="line">        [ 1.2472, -0.8905,  1.4315, -1.1237, -0.8593]])</span><br></pre></td></tr></table></figure><p>在压缩后的张量中，原始张量中维度为1的额外维度被去除，得到一个维度更少的张量。</p><p>需要注意的是，如果要指定压缩的维度，可以将<code>dim</code>参数设置为相应的维度。例如，<code>torch.squeeze(x, dim=1)</code>将只在第二个维度处进行压缩。</p><h2 id="permute"><a href="#permute" class="headerlink" title="permute()"></a>permute()</h2><p><code>permute()</code>函数是PyTorch中用于对张量进行维度重排的方法。它可以使用一个整数列表作为参数来指定新的维度顺序。这个整数列表中的每个元素表示原始张量中的相应维度在新张量中的位置。</p><p><code>permute()</code>函数的参数及其作用如下：</p><ul><li>参数：<code>*dims</code> 或 <code>[dims]</code>，一个整数列表，表示新的维度顺序。</li><li>作用：将张量的维度按照给定的顺序进行重排。</li></ul><p>下面是一个示例，展示了如何使用<code>permute()</code>函数并提供参数来重新排列张量的维度：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为(2, 3, 4)的张量</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用permute()函数进行维度重排</span></span><br><span class="line">y = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x的形状:&quot;</span>, x.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y的形状:&quot;</span>, y.shape)</span><br></pre></td></tr></table></figure><p>输出结果如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Copy Codex的形状: torch.Size([2, 3, 4])</span><br><span class="line">y的形状: torch.Size([3, 2, 4])</span><br></pre></td></tr></table></figure><p>在这个示例中，我们创建了一个形状为<code>(2, 3, 4)</code>的张量<code>x</code>。然后，使用<code>permute(1, 0, 2)</code>对原始张量进行维度重排，将维度1和维度0进行交换，得到新的张量<code>y</code>。最后，打印<code>x</code>和<code>y</code>的形状，可以看到维度顺序发生了改变。</p><p>需要注意的是，<code>permute()</code>函数返回的是一个新的张量，原始张量并没有被修改。此外，参数列表中的每个元素都必须是原始张量维度的有效索引，并且不能有重复的索引。</p><h2 id="scatter"><a href="#scatter" class="headerlink" title="scatter_()"></a>scatter_()</h2><p><code>scatter_()</code>是PyTorch中的一个张量方法，用于按照指定的索引在张量中进行原位散射（in-place scatter）。它可以根据给定的索引和值，在指定位置上更新张量的值。</p><p><code>scatter_()</code>方法的参数及其作用如下：</p><ul><li>参数：<code>dim</code>，表示要进行散射操作的维度。</li><li>参数：<code>index</code>，表示要更新的位置在指定维度上的索引。</li><li>参数：<code>src</code>，表示要填充到指定位置的源数据。</li><li>参数：<code>reduce</code>（可选），表示在指定位置处出现多个源数据时，如何对它们进行合并操作。</li><li>作用：根据给定的索引和值，在指定位置上更新张量的值。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = y.scatter(dim,index,src)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#则结果为：</span></span><br><span class="line">y[ index[i][j][k]  ] [j][k] = src[i][j][k] <span class="comment"># if dim == 0</span></span><br><span class="line">y[i] [ index[i][j][k] ] [k] = src[i][j][k] <span class="comment"># if dim == 1</span></span><br><span class="line">y[i][j] [ index[i][j][k] ]  = src[i][j][k] <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure><p>下面是一个示例，展示如何使用<code>scatter_()</code>方法来实现原位散射操作：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为(4, 3)的零张量</span></span><br><span class="line">x = torch.zeros(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要更新的索引</span></span><br><span class="line">indices = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                        [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要填充的源数据</span></span><br><span class="line">values = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在指定位置上更新张量的值</span></span><br><span class="line">x.scatter_(dim=<span class="number">0</span>, index=indices, src=values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>输出结果如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Copy Codetensor([[4, 3, 2],</span><br><span class="line">        [3, 1, 4],</span><br><span class="line">        [2, 4, 3],</span><br><span class="line">        [1, 2, 1]])</span><br></pre></td></tr></table></figure><p>在这个示例中，我们首先创建了一个形状为<code>(4, 3)</code>的零张量<code>x</code>。然后，定义了一个形状为<code>(4, 3)</code>的索引张量<code>indices</code>和一个形状为<code>(4,)</code>的源数据张量<code>values</code>。最后，我们使用<code>scatter_()</code>方法，在指定位置上更新张量<code>x</code>的值。通过设置<code>dim=0</code>，我们在第0维（行）上进行散射操作。</p><p>需要注意的是，<code>scatter_()</code>方法会原地修改张量，因此在方法名称末尾有下划线<code>_</code>。此外，如果在指定的位置上存在多个源数据，要根据具体情况选择如何合并它们，可以使用<code>reduce</code>参数来指定合并操作，默认是求和。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> ML </tag>
            
            <tag> 方法 </tag>
            
            <tag> 函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN原理精讲与代码实战</title>
      <link href="/jamin6/2023/07/11/GAN%E5%8E%9F%E7%90%86%E7%B2%BE%E8%AE%B2%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/"/>
      <url>/jamin6/2023/07/11/GAN%E5%8E%9F%E7%90%86%E7%B2%BE%E8%AE%B2%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h1 id="零、GAN原理精讲与代码实战"><a href="#零、GAN原理精讲与代码实战" class="headerlink" title="零、GAN原理精讲与代码实战"></a>零、GAN原理精讲与代码实战</h1><h1 id="一、GAN原理和结构"><a href="#一、GAN原理和结构" class="headerlink" title="一、GAN原理和结构"></a>一、GAN原理和结构</h1><h2 id="1、什么是GAN"><a href="#1、什么是GAN" class="headerlink" title="1、什么是GAN"></a>1、什么是GAN</h2><p>​        生成对抗网络（Generative Adversarial Networks，简称GAN）是当前人工智能学界最为重要的研究热点之一。其突出的生成对抗能力不仅可用于生成各类图像和自然语言数据，还启发和推动了各类半监督学习和无监督学习任务的发展。</p><p>​        GAN是一种深度神经网络架构，由一个生成网络和一个判别网络组成。生成网络产生“假”数据，并试图图欺骗判别网络;判别网络对生成数据进行真伪鉴别，试图正确识别所有”假”数据。在训练迭代的过程中，两个网络持续地进化和对抗,直到达到平衡状态(参考纳什均衡)，判别网络无法再识别“假”数据，训练结束。</p><h2 id="2、GAN原理"><a href="#2、GAN原理" class="headerlink" title="2、GAN原理"></a>2、GAN原理</h2><p>​        GAN模型主要包括了两个部分:</p><p>​        生成模型(Generative Model)和判别模型(Discriminative Model) ,也常叫做生成器(generator）与判别器(discriminator)。</p><h3 id="生成模型（生成器）"><a href="#生成模型（生成器）" class="headerlink" title="生成模型（生成器）"></a>生成模型（生成器）</h3><p>​        生成器主要用来学习真实图像分布从而让自身生成的图像更加真实，以骗过判别器。</p><h3 id="判别模型（判别器）"><a href="#判别模型（判别器）" class="headerlink" title="判别模型（判别器）"></a>判别模型（判别器）</h3><p>​        判别器则需要对接收的图片进行真假判别。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>​        在训练过程中，生成器努力地让生成的图像更加真实,而判别器则努力地去识别出图像的真假，这个过程相当于一个二人博弈，随着时间的推移，生成器和判别器在不断地进行对抗。</p><p>​        最终两个网络达到了一个动态均衡:生成器生成的图像接近于真实图像分布，而判别器识别不出真假图像,对于给定图像的预测为真的概率基本接近0.5(相当于随机猜测尖别)。</p><h2 id="3、GAN模型结构"><a href="#3、GAN模型结构" class="headerlink" title="3、GAN模型结构"></a>3、GAN模型结构</h2><p>​    模型结构如下图所示：</p><p><img src="https://cdn.staticaly.com/gh/Jamin6/picx-images-hosting@master/image-20230711154035884.y0eztbee9m8.webp" alt="image-20230711154035884"></p><h2 id="4、损失函数"><a href="#4、损失函数" class="headerlink" title="4、损失函数"></a>4、损失函数</h2><p>​        GAN设计的关键在于损失函数的处理。对于判别模型，损失函数是容易定义的，判别器主要用来判断一张图片是真实的还是生成的，显然这是一个二分类问题。对于生成模型，损失函数的定义就不是那么容易。我们希望生成器可以生成接近真实的图片，对于生成的图片是否像真实的，我们人类肉眼容易判断，但具体到代码中，往往是一个抽象的，难以数学公理化定义的范式。</p><p>​        针对这个问题，我们不妨把生成模型的输出，交给判别模型处理，让判别器来判断这是一个真实的图像还是假的图像，因为深度学习模型很适合做图片的分类。这样就将生成对抗网络中的两大类模型生成器与判别器紧密地联合在了一起。</p><h1 id="二、GAN算法流程与公式"><a href="#二、GAN算法流程与公式" class="headerlink" title="二、GAN算法流程与公式"></a>二、GAN算法流程与公式</h1><h2 id="1、算法流程"><a href="#1、算法流程" class="headerlink" title="1、算法流程"></a>1、算法流程</h2><p>​        GAN这个结构的最精妙之处在于对生成模型损失函数的处理，这里以生成图片为例，说明其整个算法流程。假设我们有两个网络：G(Generator)和D (Discriminator)。</p><ul><li>G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。</li><li>D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D(x)代表x为真实图片的概率，如果为1,就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。</li></ul><p>​        在训练过程中，将随机噪声输入生成网络G，得到生成的图片；判别器接收生成的图片和真实的图片，并尽量将两者区分开来。在这个计算过程中，能否正确区分生成的图片和真实的图片将作为判别器的损失，而能否生成近似真实的图片并使得判别器将生成的图片判定为真将作为生成器的损失。</p><p>​        <strong>注</strong>：生成器的损失是通过判别器的输出来计算的,而判别器的输出是一个概率值，我们可以通过交叉嫡计算。</p><h2 id="2、GAN公式"><a href="#2、GAN公式" class="headerlink" title="2、GAN公式"></a>2、GAN公式</h2><p>​        Goodfellow从理论上证明了GAN算法的收敛性以及在模型收敛时生成数据具有和真实数据相同的分布。<br>​        GAN的公式如图：</p><img src="https://cdn.staticaly.com/gh/Jamin6/picx-images-hosting@master/image-20230711161912533.1zrpmvuedvts.webp" alt="image-20230711161912533" style="zoom: 33%;" /><p>​        公式中x表示真实图片，z表示输入G网络的噪声，G(z)表示G网络生成的图片，D(*)表示D网络判断图片是否真实的概率。</p><ul><li>从判别器D的角度，希望最大化V(D,G)</li><li>从生成器G的角度，希望最小化V(D,G)</li></ul><h2 id="3、GAN应用领域"><a href="#3、GAN应用领域" class="headerlink" title="3、GAN应用领域"></a>3、GAN应用领域</h2><ul><li>图片生成：生成一些假数据，比如海报种的人脸</li><li>图像增强：从分割图种生成假的真实街景，方标训练无人汽车。</li><li>风格化和艺术的图像创造：转换图像风格，修补图像。</li><li>变声：一个人的声音转为另一个人的声音；去除噪声等。</li></ul><h1 id="三、GAN实战"><a href="#三、GAN实战" class="headerlink" title="三、GAN实战"></a>三、GAN实战</h1><p>这里采用Mnist数据集。</p><h2 id="1、导包"><a href="#1、导包" class="headerlink" title="1、导包"></a>1、导包</h2><p>导入常用的包：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><h2 id="2、预设置"><a href="#2、预设置" class="headerlink" title="2、预设置"></a>2、预设置</h2><p>预设置参数和设备等信息：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class="string">&quot;TRUE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设备</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">test_input = torch.randn(<span class="number">16</span>, <span class="number">100</span>, device=device)</span><br></pre></td></tr></table></figure><h2 id="3、准备数据集"><a href="#3、准备数据集" class="headerlink" title="3、准备数据集"></a>3、准备数据集</h2><p>准备数据集并做对应的处理：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line"><span class="comment"># 对数据做归一化  （-1, 1）</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment"># 该方法会做(0,1)的归一化，并将结构改为C.H.W，还会转为Tensor类型</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        <span class="comment"># 归一化  参数(均值,方差) 开始在(0,1)，要变为(-1,1)，所以参数为(0.5，0.5)</span></span><br><span class="line">        transforms.Normalize(<span class="number">0.5</span>, <span class="number">0.5</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、准备数据集</span></span><br><span class="line">train_ds = torchvision.datasets.MNIST(<span class="string">&quot;mnist_data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                     transform=transform</span><br><span class="line">                                     )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;==========【数据集加载完成】==========&quot;</span>)</span><br><span class="line">dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="4、定义生成器和判别器"><a href="#4、定义生成器和判别器" class="headerlink" title="4、定义生成器和判别器"></a>4、定义生成器和判别器</h2><p>生成器：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2、定义生成器</span></span><br><span class="line"><span class="comment">#     输入是长度为100的随机噪声（正态分布随机数）</span></span><br><span class="line"><span class="comment">#     输出为(1,28,28)的图片</span></span><br><span class="line"><span class="comment">#   linear 1: 100 -&gt; 256</span></span><br><span class="line"><span class="comment">#   linear 2: 256 -&gt; 512</span></span><br><span class="line"><span class="comment">#   linear 3: 512 -&gt; 28*28</span></span><br><span class="line"><span class="comment">#   linear 4: 28*28 -&gt; reshape(1,28,28)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># linear参数(in_channel, out_channel)</span></span><br><span class="line">            nn.Linear(<span class="number">100</span>, <span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            <span class="comment"># 在GAN中，最后一层的激活层采用Tanh效果较好(经验)</span></span><br><span class="line">            nn.Tanh()           <span class="comment"># (-1, 1)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 得到 28*28 的 序列</span></span><br><span class="line">        img = self.main(x)</span><br><span class="line">        <span class="comment"># reshape (1,28,28)</span></span><br><span class="line">        img = img.view(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><p>判别器：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3、定义判别器</span></span><br><span class="line"><span class="comment">#     输入：(1,28,28)的图片</span></span><br><span class="line"><span class="comment">#     输出：二分类的概率值（输出使用Sigmod激活 0-1）</span></span><br><span class="line"><span class="comment">#       BCELoss 计算交叉熵损失</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">        <span class="comment"># linear参数(in_channel, out_channel)</span></span><br><span class="line">        nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">        <span class="comment"># 相比ReLu，该激活方法会保留一定的梯度</span></span><br><span class="line">            <span class="comment"># f(x) :  x&gt;0 输出 x, x&lt;0 输出 a*x  a是一个很小的斜率 如0.01，0.002等</span></span><br><span class="line">        <span class="comment"># 在判别器中一般推荐使用LeakyReLU激活</span></span><br><span class="line">        nn.LeakyReLU(),</span><br><span class="line">        nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">        nn.LeakyReLU(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">        nn.Sigmoid()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        x = self.main(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="5、模型等初始化"><a href="#5、模型等初始化" class="headerlink" title="5、模型等初始化"></a>5、模型等初始化</h2><p>初始化模型、优化器和损失函数等：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4、初始化</span></span><br><span class="line"><span class="comment">#   g</span></span><br><span class="line">gen = Generator().to(device)</span><br><span class="line"><span class="comment">#   d</span></span><br><span class="line">dis = Discriminator().to(device)</span><br><span class="line"><span class="comment">#   优化器</span></span><br><span class="line">d_optim = torch.optim.Adam(dis.parameters(), lr=lr)</span><br><span class="line">g_optim = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line"><span class="comment">#   损失函数</span></span><br><span class="line">loss_fn = torch.nn.BCELoss()</span><br></pre></td></tr></table></figure><h2 id="6、绘制图像等操作"><a href="#6、绘制图像等操作" class="headerlink" title="6、绘制图像等操作"></a>6、绘制图像等操作</h2><p>编写图像绘制方法等：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5、绘制图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gen_img_plot</span>(<span class="params">model, test_input</span>):</span><br><span class="line">    prediction = np.squeeze(model(test_input).detach().cpu().numpy())</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">        plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># +1  /2  是为了将(-1,1)转为(0,1)</span></span><br><span class="line">        plt.imshow((prediction[i] + <span class="number">1</span>) / <span class="number">2</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="7、模型训练和效果查看"><a href="#7、模型训练和效果查看" class="headerlink" title="7、模型训练和效果查看"></a>7、模型训练和效果查看</h2><p>模型训练：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 6、GAN训练</span></span><br><span class="line">D_Loss = []</span><br><span class="line">G_Loss = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    d_epoch_loss = <span class="number">0</span></span><br><span class="line">    g_epoch_loss = <span class="number">0</span></span><br><span class="line">    count = <span class="built_in">len</span>(dataloader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step, (img, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        img = img.to(device)</span><br><span class="line">        size = img.size(<span class="number">0</span>)</span><br><span class="line">        random_noise = torch.randn(size, <span class="number">100</span>, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度归0</span></span><br><span class="line">        d_optim.zero_grad()</span><br><span class="line">        <span class="comment"># 对判别器输入真实图片  real_img_output 真实图片的识别结果</span></span><br><span class="line">        real_img_output = dis(img)</span><br><span class="line">        <span class="comment"># 判别器在真实图片上的损失</span></span><br><span class="line">        d_real_img_loss = loss_fn(real_img_output, torch.ones_like(real_img_output))</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        d_real_img_loss.backward()</span><br><span class="line">        <span class="comment"># 生成假图片</span></span><br><span class="line">        gen_img = gen(random_noise)</span><br><span class="line">        <span class="comment"># 输入假图片给判别器</span></span><br><span class="line">        fake_img_output = dis(gen_img.detach())</span><br><span class="line">        <span class="comment"># 判别器在假图片上的损失</span></span><br><span class="line">        d_fake_img_loss = loss_fn(fake_img_output, torch.zeros_like(real_img_output))</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        d_fake_img_loss.backward()</span><br><span class="line">        <span class="comment"># 判别器的总损失</span></span><br><span class="line">        d_loss = d_real_img_loss + d_fake_img_loss</span><br><span class="line">        <span class="comment"># 判别器，优化</span></span><br><span class="line">        d_optim.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成器梯度归0</span></span><br><span class="line">        g_optim.zero_grad()</span><br><span class="line">        fake_img_output = dis(gen_img)</span><br><span class="line">        <span class="comment"># 生成器的损失</span></span><br><span class="line">        g_loss = loss_fn(fake_img_output, torch.ones_like(real_img_output))</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        <span class="comment"># 生成器，优化</span></span><br><span class="line">        g_optim.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每一个epoch的总g_loss,d_loss</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            g_epoch_loss += g_loss</span><br><span class="line">            d_epoch_loss += d_loss</span><br><span class="line">    <span class="comment"># 每一个epoch的平均loss</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        g_epoch_loss /= count</span><br><span class="line">        d_epoch_loss /= count</span><br><span class="line">        <span class="comment"># 存放每个epoch的平均Loss</span></span><br><span class="line">        D_Loss.append(d_epoch_loss)</span><br><span class="line">        G_Loss.append(g_epoch_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch: &quot;</span>, epoch + <span class="number">1</span>)</span><br><span class="line">        gen_img_plot(gen, test_input)</span><br></pre></td></tr></table></figure><p>效果查看：</p><p><img src="https://cdn.staticaly.com/gh/Jamin6/picx-images-hosting@master/image-20230712165443632.3hk7byk9hn00.webp" alt="image-20230712165443632"></p><p><img src="https://cdn.staticaly.com/gh/Jamin6/picx-images-hosting@master/image-20230712165500497.1k1n4qg28ra8.webp" alt="image-20230712165500497"></p><p><img src="https://cdn.staticaly.com/gh/Jamin6/picx-images-hosting@master/image-20230712165524474.20valucl4hkw.webp" alt="image-20230712165524474"></p><p><img src="https://cdn.staticaly.com/gh/Jamin6/picx-images-hosting@master/image-20230712165543210.4lokli95z940.webp" alt="image-20230712165543210"></p><h1 id="四、整体代码"><a href="#四、整体代码" class="headerlink" title="四、整体代码"></a>四、整体代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class="string">&quot;TRUE&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设备</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">test_input = torch.randn(<span class="number">16</span>, <span class="number">100</span>, device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line"><span class="comment"># 对数据做归一化  （-1, 1）</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment"># 该方法会做(0,1)的归一化，并将结构改为C.H.W，还会转为Tensor类型</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        <span class="comment"># 归一化  参数(均值,方差) 开始在(0,1)，要变为(-1,1)，所以参数为(0.5，0.5)</span></span><br><span class="line">        transforms.Normalize(<span class="number">0.5</span>, <span class="number">0.5</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、准备数据集</span></span><br><span class="line">train_ds = torchvision.datasets.MNIST(<span class="string">&quot;mnist_data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                     transform=transform</span><br><span class="line">                                     )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;==========【数据集加载完成】==========&quot;</span>)</span><br><span class="line">dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、定义生成器</span></span><br><span class="line"><span class="comment">#     输入是长度为100的随机噪声（正态分布随机数）</span></span><br><span class="line"><span class="comment">#     输出为(1,28,28)的图片</span></span><br><span class="line"><span class="comment">#   linear 1: 100 -&gt; 256</span></span><br><span class="line"><span class="comment">#   linear 2: 256 -&gt; 512</span></span><br><span class="line"><span class="comment">#   linear 3: 512 -&gt; 28*28</span></span><br><span class="line"><span class="comment">#   linear 4: 28*28 -&gt; reshape(1,28,28)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># linear参数(in_channel, out_channel)</span></span><br><span class="line">            nn.Linear(<span class="number">100</span>, <span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            <span class="comment"># 在GAN中，最后一层的激活层采用Tanh效果较好(经验)</span></span><br><span class="line">            nn.Tanh()           <span class="comment"># (-1, 1)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 得到 28*28 的 序列</span></span><br><span class="line">        img = self.main(x)</span><br><span class="line">        <span class="comment"># reshape (1,28,28)</span></span><br><span class="line">        img = img.view(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义判别器</span></span><br><span class="line"><span class="comment">#     输入：(1,28,28)的图片</span></span><br><span class="line"><span class="comment">#     输出：二分类的概率值（输出使用Sigmod激活 0-1）</span></span><br><span class="line"><span class="comment">#       BCELoss 计算交叉熵损失</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">        <span class="comment"># linear参数(in_channel, out_channel)</span></span><br><span class="line">        nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">        <span class="comment"># 相比ReLu，该激活方法会保留一定的梯度</span></span><br><span class="line">            <span class="comment"># f(x) :  x&gt;0 输出 x, x&lt;0 输出 a*x  a是一个很小的斜率 如0.01，0.002等</span></span><br><span class="line">        <span class="comment"># 在判别器中一般推荐使用LeakyReLU激活</span></span><br><span class="line">        nn.LeakyReLU(),</span><br><span class="line">        nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">        nn.LeakyReLU(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">        nn.Sigmoid()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        x = self.main(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、初始化</span></span><br><span class="line"><span class="comment">#   g</span></span><br><span class="line">gen = Generator().to(device)</span><br><span class="line"><span class="comment">#   d</span></span><br><span class="line">dis = Discriminator().to(device)</span><br><span class="line"><span class="comment">#   优化器</span></span><br><span class="line">d_optim = torch.optim.Adam(dis.parameters(), lr=lr)</span><br><span class="line">g_optim = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class="line"><span class="comment">#   损失函数</span></span><br><span class="line">loss_fn = torch.nn.BCELoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、绘制图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gen_img_plot</span>(<span class="params">model, test_input</span>):</span><br><span class="line">    prediction = np.squeeze(model(test_input).detach().cpu().numpy())</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">        plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># +1  /2  是为了将(-1,1)转为(0,1)</span></span><br><span class="line">        plt.imshow((prediction[i] + <span class="number">1</span>) / <span class="number">2</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、GAN训练</span></span><br><span class="line">D_Loss = []</span><br><span class="line">G_Loss = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    d_epoch_loss = <span class="number">0</span></span><br><span class="line">    g_epoch_loss = <span class="number">0</span></span><br><span class="line">    count = <span class="built_in">len</span>(dataloader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step, (img, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        img = img.to(device)</span><br><span class="line">        size = img.size(<span class="number">0</span>)</span><br><span class="line">        random_noise = torch.randn(size, <span class="number">100</span>, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度归0</span></span><br><span class="line">        d_optim.zero_grad()</span><br><span class="line">        <span class="comment"># 对判别器输入真实图片  real_img_output 真实图片的识别结果</span></span><br><span class="line">        real_img_output = dis(img)</span><br><span class="line">        <span class="comment"># 判别器在真实图片上的损失</span></span><br><span class="line">        d_real_img_loss = loss_fn(real_img_output, torch.ones_like(real_img_output))</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        d_real_img_loss.backward()</span><br><span class="line">        <span class="comment"># 生成假图片</span></span><br><span class="line">        gen_img = gen(random_noise)</span><br><span class="line">        <span class="comment"># 输入假图片给判别器</span></span><br><span class="line">        fake_img_output = dis(gen_img.detach())</span><br><span class="line">        <span class="comment"># 判别器在假图片上的损失</span></span><br><span class="line">        d_fake_img_loss = loss_fn(fake_img_output, torch.zeros_like(real_img_output))</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        d_fake_img_loss.backward()</span><br><span class="line">        <span class="comment"># 判别器的总损失</span></span><br><span class="line">        d_loss = d_real_img_loss + d_fake_img_loss</span><br><span class="line">        <span class="comment"># 判别器，优化</span></span><br><span class="line">        d_optim.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成器梯度归0</span></span><br><span class="line">        g_optim.zero_grad()</span><br><span class="line">        fake_img_output = dis(gen_img)</span><br><span class="line">        <span class="comment"># 生成器的损失</span></span><br><span class="line">        g_loss = loss_fn(fake_img_output, torch.ones_like(real_img_output))</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        <span class="comment"># 生成器，优化</span></span><br><span class="line">        g_optim.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每一个epoch的总g_loss,d_loss</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            g_epoch_loss += g_loss</span><br><span class="line">            d_epoch_loss += d_loss</span><br><span class="line">    <span class="comment"># 每一个epoch的平均loss</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        g_epoch_loss /= count</span><br><span class="line">        d_epoch_loss /= count</span><br><span class="line">        <span class="comment"># 存放每个epoch的平均Loss</span></span><br><span class="line">        D_Loss.append(d_epoch_loss)</span><br><span class="line">        G_Loss.append(g_epoch_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch: &quot;</span>, epoch + <span class="number">1</span>)</span><br><span class="line">        gen_img_plot(gen, test_input)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 数据增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo上传教程</title>
      <link href="/jamin6/2023/07/10/Hexo%E4%B8%8A%E4%BC%A0%E6%95%99%E7%A8%8B/"/>
      <url>/jamin6/2023/07/10/Hexo%E4%B8%8A%E4%BC%A0%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo上传教程"><a href="#Hexo上传教程" class="headerlink" title="Hexo上传教程"></a>Hexo上传教程</h1><h2 id="1、准备上传文件"><a href="#1、准备上传文件" class="headerlink" title="1、准备上传文件"></a>1、准备上传文件</h2><p>​    将要上传的文件放入source文件夹下的_posts文件夹下。</p><h2 id="2、生成静态文件"><a href="#2、生成静态文件" class="headerlink" title="2、生成静态文件"></a>2、生成静态文件</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><h2 id="3、本地预览"><a href="#3、本地预览" class="headerlink" title="3、本地预览"></a>3、本地预览</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><h2 id="4、部署（同步）到Github"><a href="#4、部署（同步）到Github" class="headerlink" title="4、部署（同步）到Github"></a>4、部署（同步）到Github</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
